#!/usr/bin/env python3
"""
High-level worker management CLI.
"""
from __future__ import annotations

import argparse
import json
import os
import re
import shlex
import subprocess
import sys
import tempfile
import time
from pathlib import Path
from typing import Dict, Iterable, Optional
from urllib.parse import urlparse
from urllib.request import Request, urlopen
from datetime import datetime, timezone
from collections import Counter

ROOT_DIR = Path(__file__).resolve().parent.parent
DEFAULT_NAMESPACE = "eisberg"
DEFAULT_DEPLOYMENT = "eisberg-worker"
DEFAULT_SCALEDOBJECT = "eisberg-worker-scaler"
DEFAULT_SECRET = "eisberg-worker-secrets"
DEFAULT_KUSTOMIZE_DIR = ROOT_DIR / "worker" / "deploy" / "k8s" / "worker"
DEFAULT_LOCAL_IMAGE = "eisberg-agent:latest"
DEFAULT_LOCAL_CONTAINER = "eisberg-worker-local"
DEFAULT_KEDA_VERSION = "2.14.1"


def log(message: str) -> None:
    print(f"[eb] {message}")


def run(
    cmd: Iterable[str],
    *,
    check: bool = True,
    capture_output: bool = False,
    cwd: Optional[Path] = None,
    env: Optional[Dict[str, str]] = None,
    input_text: Optional[str] = None,
    log_command: bool = True,
) -> subprocess.CompletedProcess[str]:
    cmd_list = [str(part) for part in cmd]
    stdout = subprocess.PIPE if capture_output else None
    stderr = subprocess.PIPE if capture_output else None
    if log_command:
        log(f"$ {shlex.join(cmd_list)}")
    return subprocess.run(
        cmd_list,
        check=check,
        text=True,
        cwd=str(cwd) if cwd else None,
        env=env,
        stdout=stdout,
        stderr=stderr,
        input=input_text,
    )


def load_env_file(path: Path) -> Dict[str, str]:
    if not path.exists():
        raise FileNotFoundError(f"env file not found: {path}")
    env: Dict[str, str] = {}
    for raw_line in path.read_text(encoding="utf-8").splitlines():
        line = raw_line.strip()
        if not line or line.startswith("#"):
            continue
        if line.startswith("export "):
            line = line[7:].strip()
        if "=" not in line:
            continue
        key, value = line.split("=", 1)
        env[key.strip()] = value.strip()
    return env


def write_env_file(env: Dict[str, str]) -> Path:
    fd, name = tempfile.mkstemp(prefix="workerctl-", suffix=".env")
    path = Path(name)
    with os.fdopen(fd, "w", encoding="utf-8") as handle:
        for key in sorted(env.keys()):
            handle.write(f"{key}={env[key]}\n")
    return path


def resolve_api_url(env: Dict[str, str], scope: str) -> Optional[str]:
    if env.get("API_URL"):
        return env["API_URL"]
    supabase_url = env.get("SUPABASE_URL")
    if not supabase_url:
        return None
    base = supabase_url.rstrip("/")
    if base.endswith("/functions/v1/api"):
        candidate = base
    else:
        candidate = f"{base}/functions/v1/api"
    if scope == "local":
        parsed = urlparse(supabase_url)
        if parsed.hostname in ("127.0.0.1", "localhost") and env.get("EB_DOCKER_HOST") == "1":
            host = "host.docker.internal"
            port = f":{parsed.port}" if parsed.port else ""
            candidate = f"{parsed.scheme}://{host}{port}/functions/v1/api"
    if scope == "cluster":
        parsed = urlparse(supabase_url)
        if parsed.hostname in ("127.0.0.1", "localhost"):
            candidate = "http://supabase_kong_api:8000/functions/v1/api"
    return candidate


def resolve_queue_db_url(env: Dict[str, str], scope: str) -> Optional[str]:
    if env.get("QUEUE_DB_URL"):
        return env["QUEUE_DB_URL"]
    if scope != "cluster":
        return None
    supabase_url = env.get("SUPABASE_URL")
    if not supabase_url:
        return None
    parsed = urlparse(supabase_url)
    if parsed.hostname in ("127.0.0.1", "localhost"):
        return "postgresql://postgres:postgres@supabase_db_api:5432/postgres?sslmode=disable"
    return None


def build_env(
    env_file: Path,
    scope: str,
) -> Dict[str, str]:
    env = load_env_file(env_file)
    api_url = resolve_api_url(env, scope)
    if api_url and not env.get("API_URL"):
        env["API_URL"] = api_url
    queue_db_url = resolve_queue_db_url(env, scope)
    if queue_db_url and not env.get("QUEUE_DB_URL"):
        env["QUEUE_DB_URL"] = queue_db_url
    return env


def resolve_api_key(env: Dict[str, str]) -> Optional[str]:
    return env.get("API_KEY") or env.get("SUPABASE_SERVICE_ROLE_KEY") or env.get("SUPABASE_ANON_KEY")


def ensure_queue_db(env: Dict[str, str]) -> None:
    if env.get("QUEUE_DB_URL"):
        return
    raise SystemExit(
        "QUEUE_DB_URL is required for KEDA scaling. Set it in the env file or disable KEDA."
    )


def fetch_queue_jobs(api_url: str, api_key: Optional[str], project_id: Optional[str]) -> list[dict]:
    base = api_url.rstrip("/")
    url = f"{base}/worker/jobs"
    if project_id:
        url = f"{url}?projectId={project_id}"
    headers = {"Accept": "application/json"}
    if api_key:
        headers["Authorization"] = f"Bearer {api_key}"
    req = Request(url, headers=headers)
    with urlopen(req, timeout=10) as resp:
        body = resp.read().decode("utf-8")
        if resp.status >= 400:
            raise RuntimeError(f"API error ({resp.status}): {body}")
        data = json_load(body)
    jobs = data.get("jobs", [])
    if not isinstance(jobs, list):
        raise RuntimeError("Unexpected API response; 'jobs' is not a list.")
    return jobs


def admin_set_user_type(args: argparse.Namespace) -> None:
    env = build_env(args.env_file, args.scope)
    api_url = env.get("API_URL") or resolve_api_url(env, args.scope)
    if not api_url:
        raise SystemExit("API_URL is missing and SUPABASE_URL is not set.")
    api_key = resolve_api_key(env)
    if not api_key:
        raise SystemExit("API_KEY or SUPABASE_SERVICE_ROLE_KEY is required for admin actions.")
    url = f"{api_url.rstrip('/')}/users/{args.user_id}/type"
    payload = json.dumps({"userType": args.user_type}).encode("utf-8")
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}",
        "apikey": api_key,
    }
    req = Request(url, data=payload, headers=headers, method="POST")
    with urlopen(req, timeout=10) as resp:
        body = resp.read().decode("utf-8")
        if resp.status >= 400:
            raise RuntimeError(f"API error ({resp.status}): {body}")
        data = json_load(body)
    log(f"admin user type set: user_id={data.get('userId')} userType={data.get('userType')}")


def json_load(payload: str) -> dict:
    return json.loads(payload)


def format_age(iso_str: Optional[str]) -> str:
    if not iso_str:
        return "?"
    try:
        parsed = datetime.fromisoformat(iso_str.replace("Z", "+00:00"))
        if parsed.tzinfo is None:
            parsed = parsed.replace(tzinfo=timezone.utc)
        now = datetime.now(timezone.utc)
        delta = now - parsed
        seconds = int(delta.total_seconds())
        if seconds < 0:
            seconds = 0
        if seconds < 60:
            return f"{seconds}s"
        if seconds < 3600:
            return f"{seconds // 60}m"
        if seconds < 86400:
            return f"{seconds // 3600}h"
        return f"{seconds // 86400}d"
    except Exception:
        return "?"


def capture_kubectl_output(cmd: Iterable[str]) -> Optional[str]:
    try:
        result = run(cmd, check=False, capture_output=True, log_command=False)
    except FileNotFoundError:
        return None
    if result.returncode != 0:
        return None
    output = (result.stdout or "").strip()
    return output or None


def run_kubectl_json(cmd: Iterable[str]) -> Optional[dict]:
    raw = capture_kubectl_output(cmd)
    if not raw:
        return None
    try:
        return json.loads(raw)
    except json.JSONDecodeError:
        return None


def parse_percentage(value: str) -> Optional[float]:
    if not value:
        return None
    trimmed = value.strip()
    if trimmed.endswith("%"):
        trimmed = trimmed[:-1]
    if not trimmed:
        return None
    try:
        return float(trimmed)
    except ValueError:
        return None


def is_pod_active(pod: dict) -> bool:
    phase = pod.get("status", {}).get("phase")
    return phase not in {"Succeeded", "Failed"}


def get_namespace_pod_counts(namespace: str) -> tuple[Optional[int], Optional[int]]:
    data = run_kubectl_json(["kubectl", "-n", namespace, "get", "pods", "-o", "json"])
    if not data:
        return (None, None)
    items = data.get("items")
    if not isinstance(items, list):
        return (None, None)
    total = len(items)
    active = sum(1 for pod in items if is_pod_active(pod))
    return active, total


def get_node_counts() -> tuple[Optional[int], Optional[int]]:
    data = run_kubectl_json(["kubectl", "get", "nodes", "-o", "json"])
    if not data:
        return (None, None)
    items = data.get("items")
    if not isinstance(items, list):
        return (None, None)
    total = len(items)
    ready = 0
    for node in items:
        for condition in node.get("status", {}).get("conditions", []):
            if condition.get("type") == "Ready" and condition.get("status") == "True":
                ready += 1
                break
    return ready, total


def get_node_usage() -> tuple[Optional[float], Optional[float]]:
    raw = capture_kubectl_output(["kubectl", "top", "nodes", "--no-headers"])
    if not raw:
        return (None, None)
    cpu_values: list[float] = []
    mem_values: list[float] = []
    for line in raw.splitlines():
        parts = line.split()
        if len(parts) < 5:
            continue
        cpu_pct = parse_percentage(parts[2])
        mem_pct = parse_percentage(parts[4])
        if cpu_pct is not None:
            cpu_values.append(cpu_pct)
        if mem_pct is not None:
            mem_values.append(mem_pct)
    cpu_avg = sum(cpu_values) / len(cpu_values) if cpu_values else None
    mem_avg = sum(mem_values) / len(mem_values) if mem_values else None
    if cpu_avg is None and mem_avg is None:
        return (None, None)
    return cpu_avg, mem_avg


def describe_cluster_state(namespace: str) -> Optional[str]:
    parts: list[str] = []
    pods_active, pods_total = get_namespace_pod_counts(namespace)
    if pods_active is not None and pods_total is not None:
        parts.append(f"pods={pods_active}/{pods_total}")
    nodes_ready, nodes_total = get_node_counts()
    if nodes_ready is not None and nodes_total is not None:
        parts.append(f"nodes={nodes_ready}/{nodes_total}")
    cpu_pct, mem_pct = get_node_usage()
    if cpu_pct is not None:
        parts.append(f"cpu={cpu_pct:.1f}%")
    if mem_pct is not None:
        parts.append(f"mem={mem_pct:.1f}%")
    if not parts:
        return None
    return " ".join(parts)


def format_table_lines(headers: list[str], rows: list[list[str]]) -> list[str]:
    widths = [len(h) for h in headers]
    for row in rows:
        for idx, cell in enumerate(row):
            widths[idx] = max(widths[idx], len(cell))
    lines: list[str] = []
    header_line = "  ".join(headers[idx].ljust(widths[idx]) for idx in range(len(headers)))
    sep_line = "  ".join("-" * widths[idx] for idx in range(len(headers)))
    lines.append(header_line)
    lines.append(sep_line)
    for row in rows:
        lines.append("  ".join(row[idx].ljust(widths[idx]) for idx in range(len(headers))))
    return lines


def show_queue(args: argparse.Namespace) -> int:
    env = build_env(args.env_file, "cluster")
    api_url = env.get("API_URL") or resolve_api_url(env, "cluster")
    if not api_url:
        raise SystemExit("API_URL is missing and SUPABASE_URL is not set.")
    api_key = resolve_api_key(env)
    jobs = fetch_queue_jobs(api_url, api_key, args.project)
    jobs_sorted = sorted(
        jobs,
        key=lambda job: job.get("created_at") or "",
        reverse=True,
    )
    total = len(jobs_sorted)
    status_counts = Counter(job.get("status", "unknown") for job in jobs_sorted)
    important = ["queued", "claimed", "running", "succeeded", "failed", "killed", "unknown"]
    summary_parts = [f"total={total}"]
    for status in important:
        if status in status_counts:
            summary_parts.append(f"{status}={status_counts[status]}")
    active = sum(status_counts.get(key, 0) for key in ("queued", "claimed", "running"))
    summary_parts.append(f"active={active}")
    cluster_summary: Optional[str] = None
    if getattr(args, "scope", None) in {"cluster", "gke"}:
        cluster_summary = describe_cluster_state(args.namespace)
    rows: list[list[str]] = []
    for job in jobs_sorted[: args.limit]:
        rows.append(
            [
                str(job.get("status", "?")),
                str(job.get("job_id", "?")),
                str(job.get("project_id", "?")),
                format_age(job.get("created_at")),
            ]
        )
    lines: list[str] = []
    if cluster_summary:
        lines.append(f"Cluster: {cluster_summary}")
    lines.append(f"API: {api_url}")
    lines.append("Summary: " + " ".join(summary_parts))
    if rows:
        lines.extend(format_table_lines(["status", "job_id", "project_id", "age"], rows))
    else:
        lines.append("No jobs found.")

    for line in lines:
        print(line)
    return len(lines)


def queue_watch(args: argparse.Namespace) -> None:
    block_height = 0
    while True:
        if block_height:
            print(f"\033[{block_height}F", end="")
        printed_lines = show_queue(args)
        if block_height and printed_lines < block_height:
            diff = block_height - printed_lines
            for _ in range(diff):
                print("\033[2K")
            print(f"\033[{diff}F", end="")
        block_height = printed_lines
        sys.stdout.flush()
        time.sleep(args.interval)


def queue_handler(args: argparse.Namespace) -> None:
    if args.watch:
        queue_watch(args)
    else:
        show_queue(args)


def docker_container_exists(name: str) -> bool:
    result = run(
        ["docker", "ps", "-a", "--filter", f"name=^{name}$", "--format", "{{.Names}}"],
        check=False,
        capture_output=True,
    )
    return name in (result.stdout or "").splitlines()


def docker_container_status(name: str) -> Optional[str]:
    result = run(
        ["docker", "inspect", "-f", "{{.State.Status}}", name],
        check=False,
        capture_output=True,
    )
    if result.returncode != 0:
        return None
    return (result.stdout or "").strip()


def ensure_minikube() -> None:
    result = run(
        ["minikube", "status", "--format", "{{.Host}}"],
        check=False,
        capture_output=True,
    )
    if result.returncode != 0 or "Running" not in (result.stdout or ""):
        log("minikube not running; starting...")
        run(["minikube", "start"])


def ensure_minikube_context() -> None:
    result = run(
        ["kubectl", "config", "use-context", "minikube"],
        check=False,
        capture_output=True,
    )
    if result.returncode != 0:
        raise SystemExit("kubectl context 'minikube' not found. Run 'minikube start' first.")


def minikube_image_load(image: str) -> bool:
    result = run(
        ["minikube", "image", "load", image],
        check=False,
    )
    return result.returncode == 0


def minikube_image_build(image: str) -> None:
    run(
        [
            "minikube",
            "image",
            "build",
            "-t",
            image,
            "-f",
            "deploy/Dockerfile",
            ".",
        ],
        cwd=ROOT_DIR / "worker",
    )


def resolve_keda_version() -> str:
    taskfile = ROOT_DIR / "Taskfile.yml"
    if taskfile.exists():
        match = re.search(r"KEDA_VERSION:\s*([0-9]+(?:\.[0-9]+)*)", taskfile.read_text(encoding="utf-8"))
        if match:
            return match.group(1)
    return DEFAULT_KEDA_VERSION


def ensure_keda() -> None:
    result = run(["kubectl", "get", "ns", "keda"], check=False, capture_output=True)
    if result.returncode == 0:
        return
    keda_version = resolve_keda_version()
    keda_url = f"https://github.com/kedacore/keda/releases/download/v{keda_version}/keda-{keda_version}.yaml"
    log(f"installing KEDA {keda_version}")
    run(["kubectl", "apply", "--server-side", "--force-conflicts", "-f", keda_url])


def apply_secret(env: Dict[str, str], namespace: str, name: str) -> None:
    env_path = write_env_file(env)
    try:
        create = run(
            [
                "kubectl",
                "-n",
                namespace,
                "create",
                "secret",
                "generic",
                name,
                "--from-env-file",
                str(env_path),
                "--dry-run=client",
                "-o",
                "yaml",
            ],
            capture_output=True,
        )
        run(["kubectl", "-n", namespace, "apply", "-f", "-"], input_text=create.stdout)
    finally:
        env_path.unlink(missing_ok=True)


def ensure_namespace(namespace: str) -> None:
    result = run(["kubectl", "get", "ns", namespace], check=False, capture_output=True)
    if result.returncode == 0:
        return
    run(["kubectl", "create", "ns", namespace])


def pause_scaledobject(namespace: str, name: str, paused: bool) -> None:
    value = "true" if paused else "false"
    run(
        [
            "kubectl",
            "-n",
            namespace,
            "patch",
            "scaledobject",
            name,
            "--type",
            "merge",
            "-p",
            f'{{"metadata":{{"annotations":{{"autoscaling.keda.sh/paused":"{value}"}}}}}}',
        ],
        check=False,
    )


def apply_kustomize(kustomize_dir: Path) -> None:
    run(["kubectl", "apply", "-k", str(kustomize_dir)])


def delete_kustomize(kustomize_dir: Path) -> None:
    run(["kubectl", "delete", "-k", str(kustomize_dir)], check=False)


def kubectl_scale(namespace: str, deployment: str, replicas: int) -> None:
    run(
        [
            "kubectl",
            "-n",
            namespace,
            "scale",
            "deploy",
            deployment,
            f"--replicas={replicas}",
        ]
    )


def kubectl_restart(namespace: str, deployment: str) -> None:
    run(["kubectl", "-n", namespace, "rollout", "restart", f"deploy/{deployment}"])


def kubectl_status(namespace: str) -> None:
    run(["kubectl", "-n", namespace, "get", "pods"])
    run(["kubectl", "-n", namespace, "get", "scaledobject"], check=False)
    run(["kubectl", "-n", namespace, "get", "hpa"], check=False)


def get_deployment_image(namespace: str, deployment: str) -> Optional[str]:
    result = run(
        [
            "kubectl",
            "-n",
            namespace,
            "get",
            "deploy",
            deployment,
            "-o",
            "jsonpath={.spec.template.spec.containers[0].image}",
        ],
        check=False,
        capture_output=True,
    )
    if result.returncode != 0:
        return None
    image = (result.stdout or "").strip()
    return image or None


def is_remote_image(image: Optional[str]) -> bool:
    if not image:
        return False
    return "pkg.dev" in image or image.startswith("gcr.io/") or image.startswith("us.gcr.io/")


def resolve_latest_gke_image(args: argparse.Namespace) -> Optional[str]:
    image_repo = f"{args.region}-docker.pkg.dev/{args.project}/{args.repo}/{args.image_name}"
    result = run(
        [
            "gcloud",
            "artifacts",
            "docker",
            "tags",
            "list",
            image_repo,
            "--sort-by=~UPDATE_TIME",
            "--limit=1",
            "--format=value(tag)",
        ],
        check=False,
        capture_output=True,
    )
    if result.returncode != 0:
        return None
    raw = (result.stdout or "").strip()
    if not raw:
        return None
    tag = raw.splitlines()[0].strip()
    if not tag:
        return None
    if tag.startswith(image_repo + ":"):
        return tag
    if ":" in tag:
        return tag
    return f"{image_repo}:{tag}"


def local_deploy(args: argparse.Namespace) -> None:
    run(
        [
            "docker",
            "build",
            "-f",
            "deploy/Dockerfile",
            "-t",
            args.image,
            ".",
        ],
        cwd=ROOT_DIR / "worker",
    )


def local_spawn(args: argparse.Namespace) -> None:
    if docker_container_exists(args.container):
        raise SystemExit(f"container already exists: {args.container}")
    env = build_env(args.env_file, "local")
    env_path = write_env_file(env)
    try:
        run(
            [
                "docker",
                "run",
                "-d",
                "--name",
                args.container,
                "--env-file",
                str(env_path),
                "-v",
                f"{ROOT_DIR}:/workspace",
                "-w",
                "/workspace",
                args.image,
                "python",
                "-m", "worker.driver",
                "--workspace",
                "/tmp/worker",
            ],
            cwd=ROOT_DIR,
        )
    finally:
        env_path.unlink(missing_ok=True)


def local_stop(args: argparse.Namespace) -> None:
    run(["docker", "stop", args.container], check=False)


def local_delete(args: argparse.Namespace) -> None:
    run(["docker", "rm", "-f", args.container], check=False)


def local_restart(args: argparse.Namespace) -> None:
    local_delete(args)
    local_spawn(args)


def local_status(args: argparse.Namespace) -> None:
    status = docker_container_status(args.container)
    if not status:
        log(f"container: {args.container} (not found)")
        return
    log(f"container: {args.container} ({status})")
    run(["docker", "ps", "-a", "--filter", f"name=^{args.container}$"])


def cluster_prepare_env(args: argparse.Namespace) -> Dict[str, str]:
    env = build_env(args.env_file, "cluster")
    ensure_queue_db(env)
    return env


def cluster_deploy(args: argparse.Namespace) -> None:
    ensure_minikube()
    ensure_minikube_context()
    ensure_keda()
    local_deploy(args)
    if not minikube_image_load(args.image):
        log("minikube image load failed; building image inside minikube")
        minikube_image_build(args.image)
    env = cluster_prepare_env(args)
    ensure_namespace(args.namespace)
    apply_secret(env, args.namespace, args.secret)
    apply_kustomize(args.kustomize_dir)
    pause_scaledobject(args.namespace, args.scaledobject, paused=False)
    kubectl_restart(args.namespace, args.deployment)
    kubectl_status(args.namespace)


def cluster_spawn(args: argparse.Namespace) -> None:
    ensure_minikube()
    ensure_minikube_context()
    ensure_keda()
    env = cluster_prepare_env(args)
    ensure_namespace(args.namespace)
    apply_secret(env, args.namespace, args.secret)
    apply_kustomize(args.kustomize_dir)
    pause_scaledobject(args.namespace, args.scaledobject, paused=False)
    kubectl_scale(args.namespace, args.deployment, replicas=1)
    kubectl_status(args.namespace)


def cluster_stop(args: argparse.Namespace) -> None:
    ensure_minikube_context()
    pause_scaledobject(args.namespace, args.scaledobject, paused=True)
    kubectl_scale(args.namespace, args.deployment, replicas=0)


def cluster_restart(args: argparse.Namespace) -> None:
    ensure_minikube_context()
    pause_scaledobject(args.namespace, args.scaledobject, paused=False)
    kubectl_restart(args.namespace, args.deployment)


def cluster_refresh(args: argparse.Namespace) -> None:
    ensure_minikube()
    ensure_minikube_context()
    ensure_keda()
    env = cluster_prepare_env(args)
    ensure_namespace(args.namespace)
    apply_secret(env, args.namespace, args.secret)
    apply_kustomize(args.kustomize_dir)
    pause_scaledobject(args.namespace, args.scaledobject, paused=False)
    kubectl_restart(args.namespace, args.deployment)


def cluster_delete(args: argparse.Namespace) -> None:
    ensure_minikube_context()
    delete_kustomize(args.kustomize_dir)
    run(["kubectl", "-n", args.namespace, "delete", "secret", args.secret], check=False)


def cluster_status(args: argparse.Namespace) -> None:
    ensure_minikube_context()
    kubectl_status(args.namespace)


def gke_setup(args: argparse.Namespace) -> None:
    run(["gcloud", "config", "set", "project", args.project])
    run(
        [
            "gcloud",
            "container",
            "clusters",
            "get-credentials",
            args.cluster,
            "--region",
            args.region,
        ]
    )


def gke_bootstrap(args: argparse.Namespace) -> None:
    if args.skip_setup:
        gke_setup(args)
        return
    run(["task", "worker:gke:setup"], cwd=ROOT_DIR)


def gke_build_push(args: argparse.Namespace) -> str:
    tag = args.image_tag or time.strftime("%Y%m%d%H%M%S")
    image_uri = (
        f"{args.region}-docker.pkg.dev/{args.project}/{args.repo}/{args.image_name}:{tag}"
    )
    run(["gcloud", "auth", "configure-docker", f"{args.region}-docker.pkg.dev"])
    run(
        [
            "docker",
            "buildx",
            "build",
            "--platform",
            "linux/amd64",
            "-f",
            "deploy/Dockerfile",
            "-t",
            image_uri,
            "--push",
            ".",
        ],
        cwd=ROOT_DIR / "worker",
    )
    return image_uri


def gke_prepare_env(args: argparse.Namespace) -> Dict[str, str]:
    env = build_env(args.env_file, "cluster")
    ensure_queue_db(env)
    return env


def gke_deploy(args: argparse.Namespace) -> None:
    gke_bootstrap(args)
    image_uri = gke_build_push(args)
    env = gke_prepare_env(args)
    ensure_namespace(args.namespace)
    apply_secret(env, args.namespace, args.secret)
    apply_kustomize(args.kustomize_dir)
    run(
        [
            "kubectl",
            "-n",
            args.namespace,
            "set",
            "image",
            f"deploy/{args.deployment}",
            f"worker={image_uri}",
        ]
    )
    pause_scaledobject(args.namespace, args.scaledobject, paused=False)
    run(["kubectl", "-n", args.namespace, "rollout", "status", f"deploy/{args.deployment}"])
    kubectl_status(args.namespace)


def gke_spawn(args: argparse.Namespace) -> None:
    gke_bootstrap(args)
    current_image = get_deployment_image(args.namespace, args.deployment)
    if not is_remote_image(current_image):
        current_image = resolve_latest_gke_image(args)
    if not current_image:
        raise SystemExit("No remote image found for GKE. Run gke deploy to build/push one first.")
    env = gke_prepare_env(args)
    ensure_namespace(args.namespace)
    apply_secret(env, args.namespace, args.secret)
    apply_kustomize(args.kustomize_dir)
    run(
        [
            "kubectl",
            "-n",
            args.namespace,
            "set",
            "image",
            f"deploy/{args.deployment}",
            f"worker={current_image}",
        ]
    )
    pause_scaledobject(args.namespace, args.scaledobject, paused=False)
    kubectl_scale(args.namespace, args.deployment, replicas=1)
    kubectl_status(args.namespace)


def gke_stop(args: argparse.Namespace) -> None:
    gke_setup(args)
    pause_scaledobject(args.namespace, args.scaledobject, paused=True)
    kubectl_scale(args.namespace, args.deployment, replicas=0)


def gke_restart(args: argparse.Namespace) -> None:
    gke_setup(args)
    pause_scaledobject(args.namespace, args.scaledobject, paused=False)
    kubectl_restart(args.namespace, args.deployment)


def gke_refresh(args: argparse.Namespace) -> None:
    gke_setup(args)
    current_image = get_deployment_image(args.namespace, args.deployment)
    if not is_remote_image(current_image):
        current_image = resolve_latest_gke_image(args)
    if not current_image:
        raise SystemExit("No remote image found for GKE. Run gke deploy to build/push one first.")
    env = gke_prepare_env(args)
    ensure_namespace(args.namespace)
    apply_secret(env, args.namespace, args.secret)
    apply_kustomize(args.kustomize_dir)
    run(
        [
            "kubectl",
            "-n",
            args.namespace,
            "set",
            "image",
            f"deploy/{args.deployment}",
            f"worker={current_image}",
        ]
    )
    pause_scaledobject(args.namespace, args.scaledobject, paused=False)
    kubectl_restart(args.namespace, args.deployment)


def gke_delete(args: argparse.Namespace) -> None:
    gke_setup(args)
    delete_kustomize(args.kustomize_dir)
    run(["kubectl", "-n", args.namespace, "delete", "secret", args.secret], check=False)


def gke_status(args: argparse.Namespace) -> None:
    gke_setup(args)
    kubectl_status(args.namespace)


def add_common_args(parser: argparse.ArgumentParser, default_env_file: Path) -> None:
    parser.add_argument(
        "--env-file",
        type=Path,
        default=default_env_file,
        help="Path to env file for worker settings.",
    )
    parser.add_argument(
        "--namespace",
        default=DEFAULT_NAMESPACE,
        help="Kubernetes namespace.",
    )
    parser.add_argument(
        "--deployment",
        default=DEFAULT_DEPLOYMENT,
        help="Kubernetes deployment name.",
    )
    parser.add_argument(
        "--scaledobject",
        default=DEFAULT_SCALEDOBJECT,
        help="KEDA ScaledObject name.",
    )
    parser.add_argument(
        "--secret",
        default=DEFAULT_SECRET,
        help="Kubernetes secret name.",
    )
    parser.add_argument(
        "--kustomize-dir",
        type=Path,
        default=DEFAULT_KUSTOMIZE_DIR,
        help="Kustomize directory for the worker manifests.",
    )


def add_queue_args(parser: argparse.ArgumentParser) -> None:
    parser.add_argument(
        "--project",
        help="Filter jobs by project ID.",
    )
    parser.add_argument(
        "--limit",
        type=int,
        default=25,
        help="Max jobs to show in the table.",
    )
    parser.add_argument(
        "--watch",
        action="store_true",
        help="Refresh every second.",
    )
    parser.add_argument(
        "--interval",
        type=float,
        default=1.0,
        help="Refresh interval in seconds when --watch is set.",
    )


def add_local_args(parser: argparse.ArgumentParser) -> None:
    parser.add_argument(
        "--image",
        default=DEFAULT_LOCAL_IMAGE,
        help="Docker image tag to build/run.",
    )
    parser.add_argument(
        "--container",
        default=DEFAULT_LOCAL_CONTAINER,
        help="Docker container name.",
    )


def add_gke_args(parser: argparse.ArgumentParser) -> None:
    parser.add_argument(
        "--project",
        default=os.environ.get("GCP_PROJECT", "eisberg-ai"),
        help="GCP project ID.",
    )
    parser.add_argument(
        "--region",
        default=os.environ.get("GCP_REGION", "us-central1"),
        help="GCP region.",
    )
    parser.add_argument(
        "--cluster",
        default=os.environ.get("GKE_CLUSTER", "eisberg-workers-standard"),
        help="GKE cluster name.",
    )
    parser.add_argument(
        "--repo",
        default=os.environ.get("AR_REPO", "eisberg"),
        help="Artifact Registry repo.",
    )
    parser.add_argument(
        "--image-name",
        default=os.environ.get("WORKER_IMAGE_NAME", "worker"),
        help="Artifact Registry image name.",
    )
    parser.add_argument(
        "--image-tag",
        help="Optional image tag (default: timestamp).",
    )
    parser.add_argument(
        "--skip-setup",
        action="store_true",
        help="Skip one-time GKE setup (cluster/KEDA).",
    )


def add_admin_args(parser: argparse.ArgumentParser) -> None:
    parser.add_argument(
        "--env-file",
        type=Path,
        default=ROOT_DIR / "worker" / ".env.prod",
        help="Path to env file with SUPABASE_URL and service key.",
    )
    parser.add_argument(
        "--scope",
        choices=["local", "cluster", "gke"],
        default="cluster",
        help="Resolve API URL for local, cluster, or gke.",
    )
    parser.add_argument(
        "--user-id",
        required=True,
        help="User ID to update.",
    )
    parser.add_argument(
        "--user-type",
        choices=["user", "admin"],
        default="admin",
        help="User type to set.",
    )


def build_parser() -> argparse.ArgumentParser:
    epilog = """\
Examples:
  # Build image (local docker)
  python scripts/eb local deploy

  # Run one local worker with mock agent (set WORKER_AGENT_TYPE in env file)
  python scripts/eb local spawn

  # Show queue (local/cluster/GKE)
  python scripts/eb local queue --watch
  python scripts/eb cluster queue --watch
  python scripts/eb gke queue --watch --env-file worker/.env.prod

  # Local cluster: deploy (build + apply) vs refresh (env only)
  python scripts/eb cluster deploy
  python scripts/eb cluster --env-file worker/.env.local refresh

  # GKE: deploy (build + push) vs refresh (env only)
  python scripts/eb gke deploy
  python scripts/eb gke --env-file worker/.env.prod refresh

  # Admin: set user type
  python scripts/eb admin set-user-type --env-file worker/.env.local --scope local --user-id <uuid> --user-type admin
"""
    parser = argparse.ArgumentParser(
        description="High-level worker management (local, local cluster, GKE).",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=epilog,
    )
    subparsers = parser.add_subparsers(dest="scope", required=True)

    local = subparsers.add_parser("local", help="Manage a single local worker (Docker).")
    add_common_args(local, ROOT_DIR / "worker" / ".env.local")
    add_local_args(local)
    local_actions = local.add_subparsers(dest="action", required=True)
    local_action_specs = {
        "deploy": (local_deploy, "Build the local worker image."),
        "spawn": (local_spawn, "Run a single local worker container."),
        "stop": (local_stop, "Stop the local worker container."),
        "restart": (local_restart, "Restart the local worker container."),
        "delete": (local_delete, "Remove the local worker container."),
        "status": (local_status, "Show local worker container status."),
        "queue": (queue_handler, "Show queue status from the API."),
    }
    for name, (handler, help_text) in local_action_specs.items():
        action_parser = local_actions.add_parser(name, help=help_text, description=help_text)
        if name == "queue":
            add_queue_args(action_parser)
        action_parser.set_defaults(handler=handler)

    cluster = subparsers.add_parser("cluster", help="Manage a local K8s worker cluster (minikube).")
    add_common_args(cluster, ROOT_DIR / "worker" / ".env.local")
    add_local_args(cluster)
    cluster_actions = cluster.add_subparsers(dest="action", required=True)
    cluster_action_specs = {
        "deploy": (cluster_deploy, "Build image + apply manifests to the local cluster."),
        "spawn": (cluster_spawn, "Apply manifests without rebuilding the image."),
        "refresh": (cluster_refresh, "Reapply secrets/manifests and restart deployment."),
        "stop": (cluster_stop, "Pause autoscaling and scale to zero."),
        "restart": (cluster_restart, "Restart the deployment."),
        "delete": (cluster_delete, "Remove cluster resources and secret."),
        "status": (cluster_status, "Show cluster pods and autoscaling status."),
        "queue": (queue_handler, "Show queue status from the API."),
    }
    for name, (handler, help_text) in cluster_action_specs.items():
        action_parser = cluster_actions.add_parser(name, help=help_text, description=help_text)
        if name == "queue":
            add_queue_args(action_parser)
        action_parser.set_defaults(handler=handler)

    gke = subparsers.add_parser("gke", help="Manage the GKE worker cluster.")
    add_common_args(gke, ROOT_DIR / "worker" / ".env.prod")
    add_gke_args(gke)
    gke_actions = gke.add_subparsers(dest="action", required=True)
    gke_action_specs = {
        "deploy": (gke_deploy, "Build/push image + apply manifests to GKE."),
        "spawn": (gke_spawn, "Apply manifests using the current image."),
        "refresh": (gke_refresh, "Reapply secrets/manifests and restart deployment."),
        "stop": (gke_stop, "Pause autoscaling and scale to zero."),
        "restart": (gke_restart, "Restart the deployment."),
        "delete": (gke_delete, "Remove GKE resources and secret."),
        "status": (gke_status, "Show GKE pods and autoscaling status."),
        "queue": (queue_handler, "Show queue status from the API."),
    }
    for name, (handler, help_text) in gke_action_specs.items():
        action_parser = gke_actions.add_parser(name, help=help_text, description=help_text)
        if name == "queue":
            add_queue_args(action_parser)
        action_parser.set_defaults(handler=handler)

    admin_cmd = subparsers.add_parser("admin", help="Admin helpers (requires admin/service key).")
    admin_actions = admin_cmd.add_subparsers(dest="action", required=True)
    admin_set = admin_actions.add_parser("set-user-type", help="Set a user's admin type.", description="Set a user's admin type.")
    add_admin_args(admin_set)
    admin_set.set_defaults(handler=admin_set_user_type)

    return parser


def main() -> int:
    parser = build_parser()
    args = parser.parse_args()
    handler = getattr(args, "handler", None)
    if not handler:
        parser.print_help()
        return 1
    handler(args)
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
